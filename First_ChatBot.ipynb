{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa5390e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3a33e15a6354d10a8e4b4df118039c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eff2fc905664d04bb1a62a53b11bf14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a helpful assistant. Use the following context to answer the question.\n",
      "Context:\n",
      "Q: Rewrite this short introduction to make it more engaging.\n",
      "A: Are we doing enough to protect our data from the ever-growing risk of privacy violations? In this paper, we explore the potential implications of privacy rules and regulations on data confidentiality, equipping ourselves with the knowledge needed to keep our data secure.\n",
      "\n",
      "Q: Describe 3 ways to protect your data\n",
      "A: 1. Using strong passwords and maintaining good password hygiene, such as avoiding reusing passwords and regularly changing them.\n",
      "2. Installing and regularly updating antivirus and antiâ€“malware software. \n",
      "3. Encrypting data, such as with the encryption software or a Virtual Private Network (VPN).\n",
      "\n",
      "Q: Find an example of the given kind of data.\n",
      "A: An example of qualitative data is an opinion survey which asks respondents to rate certain aspects of a product or service on a scale from one (really bad) to five (really good). This type of data provides insight into the opinions and preferences of a group, rather than measurable data points.\n",
      "\n",
      "Question:\n",
      "what is data privacy?\n",
      "\n",
      "Answer:\n",
      "Data privacy refers to the practice of protecting personal data from unauthorized access, use, disclosure, or disruption. It involves implementing measures to ensure that data is collected, stored, and used in a secure and responsible manner, with the goal of safeguarding the privacy and confidentiality of individuals' personal information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "import faiss\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "\n",
    "# Step 2: Load a dataset of QA pairs\n",
    "ds = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")\n",
    "qa_pairs = [{\"question\": x[\"instruction\"], \"answer\": x[\"output\"]} for x in ds]\n",
    "random.shuffle(qa_pairs)\n",
    "qa_pairs = qa_pairs[:1000]  # limit for fast local use\n",
    "\n",
    "# Step 3: Embed documents using sentence-transformers\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "documents = [f\"Q: {q['question']}\\nA: {q['answer']}\" for q in qa_pairs]\n",
    "embeddings = model.encode(documents, show_progress_bar=True)\n",
    "\n",
    "# Step 4: Build FAISS vector index\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(np.array(embeddings))\n",
    "\n",
    "# Step 5: Load Phi-2 model (CPU-safe float32)\n",
    "model_id = \"microsoft/phi-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "phi_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32).cpu()\n",
    "\n",
    "# Step 6: Set up text generation pipeline\n",
    "pipe = pipeline(\"text-generation\", model=phi_model, tokenizer=tokenizer, max_new_tokens=200)\n",
    "\n",
    "# Step 7: LangChain LLM wrapper\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Step 8: LangChain Prompt Template\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"You are a helpful assistant. Use the following context to answer the question.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# Step 9: Combine prompt and LLM into a chain\n",
    "rag_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# Step 10: Final chatbot function\n",
    "def ask_chatbot(user_input, top_k=3):\n",
    "    query_emb = model.encode([user_input])\n",
    "    D, I = index.search(np.array(query_emb), k=top_k)\n",
    "    context = \"\\n\\n\".join([documents[i] for i in I[0]])\n",
    "    return rag_chain.invoke({\"context\": context, \"question\": user_input})[\"text\"]\n",
    "\n",
    "\n",
    "print(ask_chatbot(\"what is data privacy?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32870447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
